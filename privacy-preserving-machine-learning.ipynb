{"cells":[{"cell_type":"markdown","metadata":{"id":"lanysKs0Kg54"},"source":["# Part 1 Data Analysis:\n","## Understanding the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhpHF6mbvtnC"},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_61u0Cp8O7U","outputId":"37a2463e-34d8-4895-b382-c5908c7bfac1"},"outputs":[],"source":["# Listing all of the CSV files\n","csv_files = [\n","    '/content/supplies.csv',\n","    '/content/providers.csv',\n","    '/content/procedures.csv',\n","    '/content/payers.csv',\n","    '/content/payer_transitions.csv',\n","    '/content/patients.csv',\n","    '/content/organizations.csv',\n","    '/content/medications.csv',\n","    '/content/observations.csv',\n","    '/content/immunizations.csv',\n","    '/content/imaging_studies.csv',\n","    '/content/encounters.csv',\n","    '/content/conditions.csv',\n","    '/content/devices.csv',\n","    '/content/careplans.csv',\n","    '/content/allergies.csv'\n","]\n","#-------------------------------------------------------------------------------\n","# Reading all CSV files into pandas DataFrames and store them in a dictionary\n","dataframes = {}\n","for file in csv_files:\n","    key = file.split('/')[-1][:-4]  # Get only the filename without the path and '.csv' extension\n","    dataframes[key] = pd.read_csv(file)\n","#-------------------------------------------------------------------------------\n","# Displaying the patients DataFrame\n","display(dataframes['patients'].head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlbDAb8HAczR","outputId":"fa4d455a-b946-48a0-ac5c-aacc1a5adfd4"},"outputs":[],"source":["# Analyzing each DataFrame individually\n","for key, df in dataframes.items():\n","    print(f\"--- {key.upper()} DataFrame ---\")\n","#-------------------------------------------------------------------------------    \n","    # Displaying the first 10 rows of the DataFrame\n","    print(\"\\nFirst 10 rows:\")\n","    display(df.head(10))\n","#-------------------------------------------------------------------------------\n","    # Displaying the shape (number of rows and columns) of the DataFrame\n","    display(f\"\\nShape: {df.shape}\")\n","#-------------------------------------------------------------------------------\n","    # Displaying the column names and their data types\n","    print(\"\\nData types:\")\n","    display(df.dtypes)\n","#-------------------------------------------------------------------------------\n","    # Displaying basic statistics for numerical columns\n","    print(\"\\nBasic statistics:\")\n","    display(df.describe())\n","#-------------------------------------------------------------------------------\n","    # Checking for missing values in each column\n","    print(\"\\nMissing values:\")\n","    display(df.isnull().sum())\n","#-------------------------------------------------------------------------------\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"_b7P9vSmA_l7"},"source":["## Analysis: \n","### Summary of each DataFrame:\n","\n","1- SUPPLIES DataFrame:\n","* The DataFrame is empty with 0 rows and 6 columns (DATE, PATIENT, ENCOUNTER, CODE, DESCRIPTION, QUANTITY).\n","* All columns are of the 'object' data type.\n","* There are no missing values.\n","\n","2- PROVIDERS DataFrame:\n","* Contains 5855 rows and 12 columns (Id, ORGANIZATION, NAME, GENDER, SPECIALITY, ADDRESS, CITY, STATE, ZIP, LAT, LON, UTILIZATION).\n","* Data types are mostly 'object', except for 'LAT', 'LON', and 'UTILIZATION' which are 'float64' and 'int64', respectively.\n","* No missing values.\n","\n","3- PROCEDURES DataFrame:\n","* Contains 34,981 rows and 8 columns (DATE, PATIENT, ENCOUNTER, CODE, DESCRIPTION, BASE_COST, REASONCODE, REASONDESCRIPTION).\n","* Data types include 'object', 'int64', 'float64'.\n","* Missing values are present in 'REASONCODE' and 'REASONDESCRIPTION' columns.\n","\n","4- PAYERS DataFrame:\n","* Contains 10 rows and 21 columns.\n","* Data types are mostly 'object', with some 'float64' and 'int64'.\n","* Missing values are present in the columns 'ADDRESS', 'CITY', 'STATE_HEADQUARTERED', 'ZIP', and 'PHONE'.\n","\n","5- PAYER_TRANSITIONS DataFrame:\n","* Contains 3,801 rows and 5 columns (PATIENT, START_YEAR, END_YEAR, PAYER, OWNERSHIP).\n","* All columns are of the 'object' data type, except for 'START_YEAR' and 'END_YEAR' which are 'int64'.\n","* No missing values.\n","\n","6- ORGANIZATIONS DataFrame:\n","* Shape: (1119, 11) with a mix of 'object', 'float64', and 'int64' data types.\n","* Missing values: PHONE - 184, suggesting incomplete contact information for some organizations.\n","* Key numerical features: LAT (latitude), LON (longitude), REVENUE, UTILIZATION (number of services provided).\n","\n","7- MEDICATIONS DataFrame:\n","* Shape: (42989, 13) with a combination of 'object', 'float64', and 'int64' data types.\n","* Missing values: STOP - 1895 (end date of medication), REASONCODE - 11117, REASONDESCRIPTION - 11117 (both indicating reasons for medication).\n","* Key numerical features: CODE (medication identifier), BASE_COST, PAYER_COVERAGE, DISPENSES, TOTALCOST, REASONCODE.\n","\n","8- OBSERVATIONS DataFrame:\n","* Shape: (299697, 8) with mostly 'object' data types and some 'float64' and 'int64'.\n","* Missing values: ENCOUNTER - 30363, UNITS - 12735 (units of measurement for values).\n","* Key categorical features: CODE, DESCRIPTION, VALUE, UNITS, TYPE (categorizing different types of observations).\n","\n","9- IMMUNIZATIONS DataFrame:\n","* Shape: (15478, 6) with 'object', 'float64', and 'int64' data types.\n","* Missing values: None\n","* Key numerical features: CODE (unique identifier for immunizations), BASE_COST (cost of each immunization).\n","\n","10- IMAGING_STUDIES DataFrame:\n","* Shape: (855, 10) with a mix of 'object', 'float64', and 'int64' data types.\n","* Missing values: None, suggesting complete data for all imaging studies.\n","* Key numerical features: BODYSITE_CODE (unique identifier for the body site where the imaging study was performed).\n","\n","11- ENCOUNTERS DataFrame:\n","\n","* Shape: (53346, 15) with a combination of 'object', 'float64', and 'int64' data types.\n","* Missing values: REASONCODE (39569), REASONDESCRIPTION (39569) - both columns indicating reasons for encounters.\n","* Key features include DATE, PATIENT, PROVIDER, PAYER, ENCOUNTERCLASS, and TYPE.\n","\n","12- CONDITIONS DataFrame:\n","\n","* Shape: (8376, 6) with mostly 'object' data types and some 'float64' and 'int64'.\n","* Missing values: STOP (3811) - indicating the end date for a condition.\n","* Key features include DATE, PATIENT, ENCOUNTER, CODE, and DESCRIPTION.\n","\n","13- DEVICES DataFrame:\n","\n","* Shape: (78, 7) with 'object', 'float64', and 'int64' data types.\n","* Missing values: STOP (78) - indicating the end date for a device.\n","* Key features include DATE, PATIENT, ENCOUNTER, CODE, and DESCRIPTION.\n","\n","14- CAREPLANS DataFrame:\n","\n","* Shape: (3483, 9) with a mix of 'object', 'float64', and 'int64' data types.\n","* Missing values: STOP (1532) - indicating the end date for a care plan, REASONCODE (327), REASONDESCRIPTION (327) - both columns indicating reasons for care plans.\n","* Key features include DATE, PATIENT, ENCOUNTER, CODE, DESCRIPTION, and TYPE.\n","\n","15- ALLERGIES DataFrame:\n","\n","* Shape: (597, 6) with mostly 'object' data types and some 'float64' and 'int64'.\n","* Missing values: STOP (533) - indicating the end date for an allergy.\n","* Key features include DATE, PATIENT, ENCOUNTER, CODE, and DESCRIPTION."]},{"cell_type":"markdown","metadata":{"id":"P9hQzAamMdyZ"},"source":["## Initial Machine Learning Model Ideas:"]},{"cell_type":"markdown","metadata":{"id":"lG17KTrYI3DK"},"source":["##1- Clustering model or recommendation system for personalized treatments\n","\n","## Only using these dataframes? Maybe? \n","\n","## Algorithms: \n","* K-Means (Baseline)\n","* K-Means++ \n","* Hierarchical Clustering (Maybe this one is the most appropriate for our project due to the size of the dataframes)\n","\n","## Dataframes:\n","* Allergies\n","* Conditions \n","* Encounters\n","* Immunizations\n","* Medications\n","* Observations\n","* Patients\n","* Providers"]},{"cell_type":"markdown","metadata":{"id":"DLt_nVGOMW7n"},"source":["##2- Predictive modeling for medication adherence\n","\n","## Algorithms:\n","* Logistic Regression (Baseline)\n","* Decision Trees (Baseline) \n","* Random Forests\n","* Neural Networks \n","  * Specifically: (RNN or LSTM) due to Feedforward is suitable for binary outcome like adherence or non-adherence\n","\n","## Dataframes:\n","* Conditions\n","* Encounters\n","* Medications\n","* Observations\n","* Patients\n","* Providers \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeZA7nkJBFvT"},"outputs":[],"source":["# Read All CSV files\n","supplies = pd.read_csv('supplies.csv')\n","patients = pd.read_csv('patients.csv')\n","providers = pd.read_csv('providers.csv')\n","procedures = pd.read_csv('procedures.csv')\n","payers = pd.read_csv('payers.csv')\n","payer_transitions = pd.read_csv('payer_transitions.csv')\n","organizations = pd.read_csv('organizations.csv')\n","medications = pd.read_csv('medications.csv')\n","observations = pd.read_csv('observations.csv')\n","immunizations = pd.read_csv('immunizations.csv')\n","imaging_studies = pd.read_csv('imaging_studies.csv')\n","encounters = pd.read_csv('encounters.csv')\n","conditions = pd.read_csv('conditions.csv')\n","devices = pd.read_csv('devices.csv')\n","careplans = pd.read_csv('careplans.csv')\n","allergies = pd.read_csv('allergies.csv')"]},{"cell_type":"markdown","metadata":{"id":"0BE9wReYjH5u"},"source":["## Predictive Modeling for Medication Adherence\n","### Attempt 1 using Logistic Regression "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gl0tt2tRjqNW","outputId":"d0fbf0af-5fa0-44da-eef5-5a005b87cea8"},"outputs":[],"source":["# Datasets:\n","# Conditions\n","# Encounters\n","# Medications\n","# Observations\n","# Patients\n","# Providers\n","#-------------------------------------------------------------------------------\n","# Listing the required datasets\n","datasets = ['conditions.csv', 'encounters.csv', 'medications.csv', 'observations.csv', 'patients.csv', 'providers.csv']\n","\n","# Looping through each dataset and perform the desired checks\n","for dataset in datasets:\n","    df = pd.read_csv(dataset)\n","#-------------------------------------------------------------------------------    \n","    # Print statistics\n","    print(f\"Statistics for {dataset}:\")\n","    display(df.describe())\n","#-------------------------------------------------------------------------------    \n","    # Print shape\n","    display(f\"Shape for {dataset}: {df.shape}\")\n","#-------------------------------------------------------------------------------    \n","    # Print head\n","    print(f\"Head for {dataset}:\")\n","    display(df.head())\n","#-------------------------------------------------------------------------------    \n","    # Check for null values\n","    print(f\"{dataset} null values\")\n","    print(df.isnull().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdETz6jOkoEs","outputId":"c71b09e5-9b41-4f83-efff-967a41a97557"},"outputs":[],"source":["datasets = ['conditions.csv', 'encounters.csv', 'medications.csv', 'observations.csv', 'patients.csv']    \n","for dataset in datasets:\n","  df = pd.read_csv(dataset)\n","  print(f\"{dataset} null values\")\n","  print(df.shape)\n","  print(df.isnull().sum())\n","  print(df.dtypes)"]},{"cell_type":"markdown","metadata":{"id":"J42mjuDt253X"},"source":["- Patient Names, Conditions"]},{"cell_type":"markdown","metadata":{"id":"jMQsVX0ZDVKN"},"source":["# Part 2 Significant Revision & Modeling\n","# Where to Start the Final Project (Without Privacy Preserving)"]},{"cell_type":"markdown","metadata":{"id":"AETZwoOEO9Bv"},"source":["Following an in-depth analysis, we have concluded that the synthetic medical data is not suitable for our project due to the time constraints associated with cleaning the data. Instead, we have identified a dataset of medical costs and have decided to incorporate Social Security information from the Patients dataset. This will enable us to focus on Privacy-Preserving Machine Learning.\n","\n","https://www.kaggle.com/datasets/mirichoi0218/insurance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oKtSftBBaHaR"},"outputs":[],"source":["import pandas as pd \n","import numpy as np\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oizv1E1-FSzV"},"outputs":[],"source":["def data_tester(df):\n","    display(df)\n","    print()\n","\n","    shape = df.shape\n","    display(shape)\n","    print()\n","\n","    display('Missing Values:')\n","    missing_values = df.isnull().sum()\n","    display(missing_values)\n","    print()\n","\n","    display('Data Types:')\n","    data_types = df.dtypes\n","    display(data_types)\n","    print()\n","\n","    return df, shape, missing_values, data_types"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWlSsb_KnbeK"},"outputs":[],"source":["patients = pd.read_csv('/content/patients.csv')\n","insurance = pd.read_csv('/content/insurance.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbTQcW3_GNn0","outputId":"12774de3-c8e1-4fca-fe60-f2f5ed3d20ea"},"outputs":[],"source":["data_tester(patients)\n","print()\n","data_tester(insurance)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJoBXj5pGSRn"},"outputs":[],"source":["# Selecting a subset of rows from the insurance dataset to match the num of rows \n","# in the Patients dataset\n","sub_insurance = insurance.sample(n=len(patients), random_state=42)\n","\n","# Reseting the index of the sub_insurance to make sure it doesnt get our of order\n","sub_insurance.reset_index(drop=True, inplace=True)\n","\n","# Adding the SSN column from the Patients dataset to the sub_insurance\n","sub_insurance['SSN'] = patients['SSN']\n","\n","# Saving the new dataset\n","sub_insurance.to_csv(\"insurance_ssn.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"odsfJid0IR4m","outputId":"ce8a1133-a2ec-40fa-e8d4-f080301f3df5"},"outputs":[],"source":["data_tester(sub_insurance)"]},{"cell_type":"markdown","metadata":{"id":"UkOLx5fWWoT0"},"source":["## Extra Steps Before Modeling (Not Necessary) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y031Q_L-WumR"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","# Initialize the scaler\n","scaler = StandardScaler()\n","\n","# Scale the numerical columns\n","numerical_columns = ['age', 'bmi', 'children']\n","sub_insurance[numerical_columns] = scaler.fit_transform(sub_insurance[numerical_columns])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSOAB_lMWwu0","outputId":"701dd224-77fb-4d25-8a01-db36f0295699"},"outputs":[],"source":["import seaborn as sns\n","\n","# Calculate the correlation matrix\n","corr_matrix = sub_insurance.corr()\n","\n","# Visualize the correlation matrix using a heatmap\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n","plt.title(\"Correlation Matrix Heatmap\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"57jhU3HGX4oI"},"source":["Due to the positive correlation between charges and age, bmi and children are weak there is no issue for multicollinearity in the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUApCRZIYOOX","outputId":"b9931dc0-45cf-4a5f-878d-ea3eacebf80a"},"outputs":[],"source":["# Plot boxplots for numerical columns\n","fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","for i, col in enumerate(numerical_columns):\n","    sns.boxplot(x=sub_insurance[col], ax=axes[i])\n","    axes[i].set_title(f\"Boxplot of {col}\")\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"axzonapTYeNz","outputId":"5cbde1d7-e037-4f79-96f5-4a76757d3dcb"},"outputs":[],"source":["numerical_columns = ['age', 'bmi', 'children']\n","summary_stats = sub_insurance[numerical_columns].describe()\n","print(summary_stats)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGpL09bQYjbr","outputId":"0a5ab876-41f5-4e6b-b945-a6a9fd57d77b"},"outputs":[],"source":["iqr_stats = summary_stats.loc[['25%', '75%']].T\n","iqr_stats['IQR'] = iqr_stats['75%'] - iqr_stats['25%']\n","iqr_stats['lower_bound'] = iqr_stats['25%'] - 1.5 * iqr_stats['IQR']\n","iqr_stats['upper_bound'] = iqr_stats['75%'] + 1.5 * iqr_stats['IQR']\n","print(iqr_stats)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ppKU-uiZnqS","outputId":"673b4599-794b-4dc7-8304-7715a8fdfae8"},"outputs":[],"source":["from scipy.stats.mstats import winsorize\n","\n","# Winsorize the 'age' column to handle outliers\n","sub_insurance['age'] = winsorize(sub_insurance['age'], limits=[0.05, 0.05])\n","\n","# Check the summary statistics for the 'age' column after winsorization\n","print(sub_insurance['age'].describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzDE0TaiZzv2"},"outputs":[],"source":["# Calculate IQR and lower/upper bounds for bmi column\n","bmi_iqr = np.percentile(sub_insurance['bmi'], 75) - np.percentile(sub_insurance['bmi'], 25)\n","bmi_lower_bound = np.percentile(sub_insurance['bmi'], 25) - 1.5 * bmi_iqr\n","bmi_upper_bound = np.percentile(sub_insurance['bmi'], 75) + 1.5 * bmi_iqr\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVZMZk7NZsah"},"outputs":[],"source":["# Winsorize the children column\n","winsorized_children = winsorize(sub_insurance['children'], limits=(0.05, 0.05))\n","\n","# Replace the original children column with the winsorized values\n","sub_insurance['children'] = winsorized_children\n"]},{"cell_type":"markdown","metadata":{"id":"ADKmrmPmehdC"},"source":["## Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qq8cd84OIV0Q","outputId":"074c7bf5-f2fe-4109-898c-9ee69aaf352c"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Encode categorical variables and drop the 'SSN' column\n","# SSN column drop due to irrelevance \n","df_encoded = pd.get_dummies(sub_insurance.drop('SSN', axis=1), columns=['sex', 'smoker', 'region'], drop_first=True)\n","\n","# Separate features (X) and target (y)\n","X = df_encoded.drop('charges', axis=1)\n","y = df_encoded['charges']\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create and fit the Linear Regression Model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Predict charges for the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model's performance\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f\"Mean Squared Error: {mse:.2f}\")\n","print(f\"Root Mean Squared Error: {rmse:.2f}\")\n","print(f\"R-squared: {r2:.2f}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3ZT1DYtoLF4H"},"source":["**Explanation:**\n","The model's R-squared value is 0.76, which suggests that it can explain about 76% of the variance in the insurance charges data. This indicates a relatively good fit, as the model is able to capture a significant portion of the relationship between the input features and the target variable.\n","\n","However, the model's Root Mean Squared Error (RMSE) is 5,620.58. This means that, on average, the model's predictions are approximately $5,620.58 away from the actual insurance charges. Although the model is able to explain a substantial proportion of the variance in the data, there is still room for improvement, as the error in the predictions can be quite large in some cases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOssXEtvKQhD","outputId":"55da4c87-ff73-4592-855a-818a02173fa9"},"outputs":[],"source":["!pip install lazypredict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"to_7PLdXKYOS","outputId":"62c74434-cee1-43f5-f3a9-f7b519dee2fd"},"outputs":[],"source":["import lazypredict\n","from lazypredict.Supervised import LazyRegressor\n","from sklearn.model_selection import train_test_split\n","\n","# SSN column drop due to irrelevance \n","df_encoded = pd.get_dummies(sub_insurance.drop('SSN', axis=1), columns=['sex', 'smoker', 'region'], drop_first=True)\n","\n","# Splitting the dataset into training and testing sets\n","X = df_encoded.drop('charges', axis=1)\n","y = df_encoded['charges']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Fitting and evaluate multiple models using LazyRegressor\n","reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n","models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n","\n","# Displaying the performance of each model\n","display(models)\n"]},{"cell_type":"markdown","metadata":{"id":"4J7w9XQqLhAT"},"source":["**Without the Extra Steps:**\n","GradientBoostingRegressor has an R-squared value of 0.86 and an RMSE of 4224.49, which indicates a stronger fit and smaller prediction errors compared to the LinearRegression model.\n","<br> <br>\n","**With the Extra Steps:**\n","GradientBoostingRegressor has an R-squared value of 0.87 and an RMSE of 4193.79"]},{"cell_type":"markdown","metadata":{"id":"SAGTcOvqL2vs"},"source":["## Side Note: \n","**(Without the Extra Steps)** This is without any tuning hyperparameters, or performing feature engineering. (Our Baseline Model)\n","\n","**(With the Extra Steps)** Improved Model, but not Perfect!\n","<br> <br>\n","We can now start the Privacy Preserving Techniques."]},{"cell_type":"markdown","metadata":{"id":"u8exmB8lVhxY"},"source":["# Part 3\n","# Testing Privacy Preserving Methods"]},{"cell_type":"markdown","metadata":{"id":"b4EpJZzyoFu8"},"source":["## Where to Start the Final Project (With Privacy Preserving)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rINQWaefolXe","outputId":"c4807633-f397-4437-b846-4754904420cc"},"outputs":[],"source":["!pip install diffprivlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlTvKYSvobrF","outputId":"8d75b70b-106f-43b9-e2f5-c5f56e3d940d"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from diffprivlib.models import LinearRegression\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from numpy.linalg import norm\n","import hashlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6_gVE4Qo3nB"},"outputs":[],"source":["df = pd.read_csv('/content/insurance_ssn.csv')"]},{"cell_type":"markdown","metadata":{"id":"qzcV-fs_6ihj"},"source":["## Simulating External Hard Disk (Without Encryption)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIdnmf9fJzc8"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HONEFMdYKCtZ"},"outputs":[],"source":["def data_tester(df):\n","    display(df)\n","    print()\n","\n","    shape = df.shape\n","    display(shape)\n","    print()\n","\n","    display('Missing Values:')\n","    missing_values = df.isnull().sum()\n","    display(missing_values)\n","    print()\n","\n","    display('Data Types:')\n","    data_types = df.dtypes\n","    display(data_types)\n","    print()\n","\n","    return df, shape, missing_values, data_types"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o66nWi96cIZB"},"outputs":[],"source":["df = pd.read_csv('/content/insurance_ssn.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FO0ZG59NcX61","outputId":"19cb93e2-fc37-4ae5-81be-f0507b4e8ea3"},"outputs":[],"source":["data_tester(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUnRAWxJbnf2"},"outputs":[],"source":["# Dictionary simulating the external storage\n","external_storage = {\n","    'admin_key': '/content/insurance_ssn.csv',\n","    'insurance_key': '/content/insurance_ssn.csv',\n","    'doctor_key': '/content/insurance_ssn.csv',\n","}\n","# Admin key has full access to the dataset\n","# Insurance providers have access to SSN and medical charges only\n","# Doctors don't require access to SSN and medical charges, but need everything else\n","def read_external_storage(key):\n","    if key in external_storage:\n","        file_path = external_storage[key]\n","        df = pd.read_csv(file_path)\n","                \n","        if key == 'admin_key':\n","            return df\n","        elif key == 'insurance_key':\n","            return df[['SSN', 'charges']]\n","        elif key == 'doctor_key':\n","            return df.drop(['SSN', 'charges'], axis=1)\n","        else:\n","            raise ValueError(f\"Invalid key: {key}\")\n","    else:\n","        raise ValueError(f\"Invalid key: {key}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b3CArO51czJk","outputId":"6038ffcd-906b-4eb4-e540-e7f6d11c14dc"},"outputs":[],"source":["# Testing the admin key \n","admin = read_external_storage('admin_key')\n","display(admin)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CYlNjfsTczWj","outputId":"d5e97797-e0e9-420d-d1cf-458457bc6c82"},"outputs":[],"source":["# Testing the insurance key \n","insurance = read_external_storage('insurance_key')\n","display(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uqpn3Gu2czfz","outputId":"5b14d4a5-85b1-4856-e641-d7678d14b0fb"},"outputs":[],"source":["# Testing the doctor key \n","doctor = read_external_storage('doctor_key')\n","display(doctor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNQ60E2fdyFL","outputId":"581fbb5c-33be-441b-9b76-6a3341ec5900"},"outputs":[],"source":["# Testing a random key\n","security = read_external_storage('security_key')\n","display(security)"]},{"cell_type":"markdown","metadata":{"id":"n8whniaM8IFF"},"source":["## Simulating External Hard Disk (with Differential Privacy \"Noise\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fn-nqmxG8QA2"},"outputs":[],"source":["# Dictionary simulating the external storage\n","external_storage = {\n","    'admin_key': '/content/insurance_ssn.csv',\n","    'insurance_key': '/content/insurance_ssn.csv',\n","    'doctor_key': '/content/insurance_ssn.csv',\n","}\n","\n","def add_noise(data, epsilon):\n","    # Calculating the scale of Laplace noise\n","    # Adding Laplace noise to datapoint\n","\n","    scale = 1 / epsilon\n","\n","    noisy_data = data.copy()\n","    for column in noisy_data.columns:\n","        if noisy_data[column].dtype == 'object':\n","            # Converting string column to integer\n","            try:\n","                noisy_data[column] = noisy_data[column].astype(int)\n","            except ValueError:\n","                # Skip adding noise to non-numeric columns\n","                continue\n","\n","        noisy_data[column] = noisy_data[column].apply(lambda x: x + np.random.laplace(0, scale))\n","    \n","    return noisy_data\n","\n","def read_external_storage_with_noise(key, epsilon=0.1):\n","    if key in external_storage:\n","        file_path = external_storage[key]\n","        df = pd.read_csv(file_path)\n","        \n","        # Admin key has full access to the dataset\n","        if key == 'admin_key':\n","            return df\n","        # Insurance providers have access to SSN and medical charges only\n","        elif key == 'insurance_key':\n","            data = df[['SSN', 'charges']]\n","            noisy_data = add_noise(data, epsilon)\n","            return noisy_data\n","        # Doctors don't require access to SSN and medical charges, but need everything else\n","        elif key == 'doctor_key':\n","            data = df.drop(['SSN', 'charges'], axis=1)\n","            noisy_data = add_noise(data, epsilon)\n","            return noisy_data\n","        else:\n","            raise ValueError(f\"Invalid key: {key}\")\n","    else:\n","        raise ValueError(f\"Invalid key: {key}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrMT7scxCjFv","outputId":"38b7c192-4f7f-4e5a-b9be-4de3e9fcde72"},"outputs":[],"source":["# Testing the insurance key with and without noise\n","# Displaying both Datasets\n","\n","insurance = read_external_storage('insurance_key')\n","insurance_with_noise = read_external_storage_with_noise('insurance_key', epsilon=0.0001)\n","\n","# Using Concatenate the original and noisy data\n","# Adding column level to differentiate both datasets\n","insurance_con = pd.concat([insurance, insurance_with_noise], axis=1)\n","insurance_con.columns = pd.MultiIndex.from_product([['Original', 'Noisy'], insurance.columns])\n","\n","display(insurance_con)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwUwH6oaHv--","outputId":"396926c5-af29-4df6-9261-2ea24aafa9fa"},"outputs":[],"source":["# Extract charges from original and noisy data\n","original_charges = insurance['charges']\n","noisy_charges = insurance_with_noise['charges']\n","\n","# A scatter plot\n","plt.figure(figsize=(12, 8))\n","\n","# Plot the original charges\n","plt.scatter(range(len(original_charges)), original_charges, alpha=0.5, color='blue', marker='o', label='Original Charges')\n","\n","# Plot the noisy charges\n","plt.scatter(range(len(noisy_charges)), noisy_charges, alpha=0.5, color='red', marker='x', label='Noisy Charges')\n","\n","plt.xlabel('Data Point Index')\n","plt.ylabel('Charges')\n","plt.title('Comparison of Original and Noisy Charges')\n","\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"oQ54MbJTFIyj"},"source":["#### **Question:**\n","Should we add noise to the SSN? We might have to split the SSN to treat it as three seperate parts then adding the noise on each part"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8GPISKNIban","outputId":"94810c5f-ddb1-4537-cbf0-50fa43015101"},"outputs":[],"source":["# Testing the doctor key with and without noise\n","# Displaying both Datasets\n","\n","doctor = read_external_storage('doctor_key')\n","doctor_with_noise = read_external_storage_with_noise('doctor_key', epsilon=0.01)\n","\n","# Using Concatenate the original and noisy data\n","# Adding column level to differentiate both datasets\n","doctor_con = pd.concat([doctor, doctor_with_noise], axis=1)\n","doctor_con.columns = pd.MultiIndex.from_product([['Original', 'Noisy'], doctor.columns])\n","\n","display(doctor_con)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWDRH1AfBlv8","outputId":"445603de-5638-4b03-c9a7-06e67801025a"},"outputs":[],"source":["# Reading both original and noisy data using doctor_key\n","# Combine original and noisy data into a single dataframe\n","\n","original_data = read_external_storage('doctor_key')\n","noisy_data = read_external_storage_with_noise('doctor_key', epsilon=0.01)\n","\n","original_data['Dataset'] = 'Original'\n","noisy_data['Dataset'] = 'Noisy'\n","combined_data = pd.concat([original_data, noisy_data])\n","\n","# A scatter plot matrix\n","sns.pairplot(combined_data, hue='Dataset', diag_kind='hist', markers=['o', 's'], plot_kws={'alpha': 0.5})\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"NtlmXvS5KbFn"},"source":["## Simulating External Hard Disk (with Privacy Preserving ML \"Encryption\") Not Done"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCbN_VKrLA4d"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"h9lWSm-9VY-J"},"source":["## Pseudonymization: Replacing the SSN with pseudonyms using a a secure hash function"]},{"cell_type":"markdown","metadata":{"id":"FULU_9mDvaOq"},"source":["For example, to obtain the digest of the byte string b\"Nobody inspects the spammish repetition\":\n","\n",">>>\n","\n","\n","```\n","import hashlib\n","m = hashlib.sha256()\n","m.update(b\"Nobody inspects\")\n","m.update(b\" the spammish repetition\")\n","m.digest()\n","b'\\x03\\x1e\\xdd}Ae\\x15\\x93\\xc5\\xfe\\\\\\x00o\\xa5u+7\\xfd\\xdf\\xf7\\xbcN\\x84:\\xa6\\xaf\\x0c\\x95\\x0fK\\x94\\x06'\n","m.hexdigest()\n","'031edd7d41651593c5fe5c006fa5752b37fddff7bc4e843aa6af0c950f4b9406'\n","```\n","More condensed:\n","```\n","hashlib.sha256(b\"Nobody inspects the spammish repetition\").hexdigest()\n","'031edd7d41651593c5fe5c006fa5752b37fddff7bc4e843aa6af0c950f4b9406'\n","```\n","\n","Reference Link: https://docs.python.org/3/library/hashlib.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sODKbMpLuiMd"},"outputs":[],"source":["import hashlib\n","\n","def pseudonymize_ssn(data):\n","    for i in range(len(data)):\n","        data.loc[i, 'SSN'] = hashlib.sha256(data.loc[i, 'SSN'].encode()).hexdigest()\n","    return data\n","\n","data = pseudonymize_ssn(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okfqtKJ3upUu","outputId":"6e76711a-535d-4f27-aed1-7b525d2b49b2"},"outputs":[],"source":["data.head(20)"]},{"cell_type":"markdown","metadata":{"id":"AzGJevIMw4I5"},"source":["## Applying Differential Privacy & Pseudonymization to a Linear Regression Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvxqfe74xG__","outputId":"0b651f7b-7797-4531-b2db-1cb420d96745"},"outputs":[],"source":["!pip install diffprivlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjzg3YEhyWSa"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from diffprivlib.models import LinearRegression\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from numpy.linalg import norm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVh6LO7wyBIe","outputId":"b0502dbc-7a4b-4293-bc84-046ffc30d2bb"},"outputs":[],"source":["data = pd.read_csv('insurance_ssn.csv')\n","data_tester(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Af6yVig1PG9"},"outputs":[],"source":["import hashlib\n","\n","def pseudonymize_ssn(data):\n","    for i in range(len(data)):\n","        data.loc[i, 'SSN'] = hashlib.sha256(data.loc[i, 'SSN'].encode()).hexdigest()\n","    return data\n","data = pseudonymize_ssn(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2XoLl8W1dL2","outputId":"3fd06407-8dc7-4e46-b97e-70800d17488e"},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXor56Ma0vyy","outputId":"ee745da4-b500-47cb-e748-407e9aa2a9e2"},"outputs":[],"source":["# Calculate the L2 norm of each row in the data\n","l2_norms = norm(X, axis=1)\n","\n","# Get the maximum L2 norm\n","max_l2_norm = np.max(l2_norms)\n","\n","print('Max L2 Norm:', max_l2_norm)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRaLAiMs0YaN","outputId":"745887dd-38c5-4870-8a5d-6c2264b08097"},"outputs":[],"source":["# Initializing the scaler\n","scaler = StandardScaler()\n","\n","# Scaling the numerical columns\n","numerical_columns = ['age', 'bmi', 'children', 'charges']\n","data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n","\n","# Creating dummy variables for categorical columns\n","df_encoded = pd.get_dummies(data, columns=['sex', 'smoker', 'region'], drop_first=True)\n","\n","# Separating features (X) and target (y)\n","X = df_encoded.drop(['charges', 'SSN'], axis=1) # we drop 'SSN' because it doesn't provide useful information for our model\n","y = df_encoded['charges']\n","\n","# Split the dataset into training and test sets\n","train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Applying differential privacy from diffprivlib.models\n","model = LinearRegression(epsilon=1.0, data_norm=4.383231988180274)\n","model.fit(train_X, train_y)\n","\n","# Predicting on the test set\n","pred_y = model.predict(test_X)\n","\n","# Evaluating the model\n","mse = mean_squared_error(test_y, pred_y)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(test_y, pred_y)\n","\n","print(f\"Mean Squared Error: {mse:.2f}\")\n","print(f\"Root Mean Squared Error: {rmse:.2f}\")\n","print(f\"R-squared: {r2:.2f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"Z1DgQdfnPj8n"},"source":["## Applying Federated Learning with Diagram\n"]},{"cell_type":"markdown","metadata":{"id":"qt18NXkM9ngI"},"source":["### Using a Feed Forward Neural Network Model\n","### A Centralized Model: \n","that consists of a single coordinating organization, called the federation owner or orchestrator, and a set of participant organizations or data owners."]},{"cell_type":"markdown","metadata":{"id":"rXL-NHDp8DWD"},"source":["An Overview of Federated Learning: https://www.youtube.com/watch?v=1YbPmkChcbo <br>\n","Initial thoughts are:\n","- Perfect for TinyML\n","- Great for multiple clients with multiple devices\n","- Outdated 2019  \n","<br>\n","\n","A Image Classification Tutorial:\n","https://github.com/tensorflow/federated/blob/main/docs/tutorials/federated_learning_for_image_classification.ipynb <br>\n","A link to understand this Module: tff.learning.algorithms.build_weighted_fed_avg\n","https://www.tensorflow.org/federated/api_docs/python/tff/learning\n","<br>\n","A link to showcase the building block in building your own federated learning simulation/ real scenario:\n","https://www.tensorflow.org/federated/tutorials/building_your_own_federated_learning_algorithm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGrsZi_CAtEU"},"outputs":[],"source":["import pandas as pd\n","import matplotlib as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BqSeOzY6rzqU"},"outputs":[],"source":["df = pd.read_csv('/content/insurance_ssn.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZbB8PG1trtc8","outputId":"99bcd54e-948f-4613-9940-9af5a9e97bf8"},"outputs":[],"source":["data_tester(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERVWDVcl0g3a","outputId":"0926b7e2-6b90-43b4-8780-dac4ac0805be"},"outputs":[],"source":["# Had to uninstall and re-install tensorflow_federated for it to work\n","!pip uninstall -y tensorflow_federated\n","!pip install tensorflow_federated"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSsSFLy30x0E","outputId":"4de2d8c7-c990-4853-d068-e67ff5b7a19e"},"outputs":[],"source":["# Restarted the runtime and re-installed tensorflow_federated\n","!pip install tensorflow_federated"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dq85CeFbzuAZ","outputId":"74af8b7d-7127-4a1a-bd9b-8669f7091b65"},"outputs":[],"source":["# Checking the version of Python\n","!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehsAq8RPumBR"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_federated as tff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0faC1K20kM1","outputId":"afabfb2c-c84c-40ff-996e-254c81429d35"},"outputs":[],"source":["# Printing the version of the \"tff\" package\n","print(tff.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LnS9CIywUNR","outputId":"f30ae17a-cd16-4112-9b18-4419d02323c4"},"outputs":[],"source":["df['sex'] = df['sex'].map({'female': 0, 'male': 1})\n","df['smoker'] = df['smoker'].map({'no': 0, 'yes': 1})\n","df['region'] = df['region'].map({'northeast': 0, 'northwest': 1, 'southeast': 2, 'southwest': 3})\n","df['client_id'] = df['SSN'].apply(lambda x: int(x[-1]))\n","\n","# One-hot encode the 'region' column (Categorical Data)\n","df = pd.get_dummies(df, columns=['region'])\n","\n","# Partition the data into client datasets to simulate multiple devices \n","client_datasets = []\n","for client_id in df['client_id'].unique():\n","    client_df = df[df['client_id'] == client_id]\n","    features = client_df.drop(columns=['charges', 'SSN', 'client_id']).values\n","    labels = client_df['charges'].values\n","    client_tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(1)\n","    client_datasets.append(client_tf_dataset)\n","\n","client_datasets = []\n","for client_id in client_ids:\n","    client_df = df[df['SSN'] == client_id]\n","    \n","    # Ensure that there is more than one record for each client\n","    if len(client_df) > 1:\n","        features = client_df.drop(columns=['charges', 'SSN']).values\n","        labels = client_df['charges'].values\n","        client_tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n","        \n","        # Batch the dataset its adjustable if needed\n","        client_tf_dataset = client_tf_dataset.batch(1) \n","        client_datasets.append(client_tf_dataset)\n","\n","\n","# A keras model\n","def create_keras_model():\n","    return tf.keras.models.Sequential([\n","        tf.keras.layers.Dense(10, input_shape=(6,), activation='relu'),  \n","        tf.keras.layers.Dense(1)\n","    ])\n","\n","# The Keras model in a TFF learning model\n","def model_fn():\n","    keras_model = create_keras_model()\n","    return tff.learning.models.from_keras_model(\n","        keras_model,\n","        input_spec=client_datasets[0].element_spec,\n","        loss=tf.keras.losses.MeanSquaredError(),\n","        metrics=[tf.keras.metrics.MeanSquaredError()]\n","    )\n","\n","# Defining the federated averaging process (Averaging Client to Server)\n","\n","federated_averaging = tff.learning.algorithms.build_weighted_fed_avg(\n","    model_fn,\n","    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n","    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",")\n","\n","# Running the federated averaging process\n","state = federated_averaging.initialize()\n","for _ in range(10):\n","    state, metrics = federated_averaging.next(state, client_datasets)\n","    print('metrics:', metrics)"]},{"cell_type":"markdown","metadata":{"id":"LwDAZuXVkqDt"},"source":["The reason why the the code did not work (input_shape=(6,) instead of input_shape=(9,) due to get.dummies )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDIVElbt7IaL","outputId":"d642eceb-c3f0-4262-9af9-9c9c7e582e8c"},"outputs":[],"source":["# Wont work without the it being Pre-proceesed\n","df['sex'] = df['sex'].map({'female': 0, 'male': 1})\n","df['smoker'] = df['smoker'].map({'no': 0, 'yes': 1})\n","df['region'] = df['region'].map({'northeast': 0, 'northwest': 1, 'southeast': 2, 'southwest': 3})\n","\n","# Create client IDs based on the last digit of SSN to simplify the process\n","# Instead of splitting the ssn to three different categories\n","df['client_id'] = df['SSN'].apply(lambda x: int(x[-1]))\n","\n","# One-hot encode the 'region' column (Categorical Data)\n","df = pd.get_dummies(df, columns=['region'])\n","\n","# The unique client IDs\n","client_ids = df['client_id'].unique()\n","\n","# Partition the data into client datasets to simulate multiple devices \n","client_datasets = []\n","for client_id in client_ids:\n","    client_df = df[df['client_id'] == client_id]\n","    \n","    # Ensure that there is more than one record for each client\n","    if len(client_df) > 1:\n","        features = client_df.drop(columns=['charges', 'SSN', 'client_id']).values\n","        labels = client_df['charges'].values\n","        client_tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n","        \n","        # Batch the dataset its adjustable if needed\n","        client_tf_dataset = client_tf_dataset.batch(1)\n","        client_datasets.append(client_tf_dataset)\n","\n","\n","# A keras model\n","# Reason why the previous code did not work (input_shape=(9,) due to get.dummies)\n","def create_keras_model():\n","    return tf.keras.models.Sequential([\n","        tf.keras.layers.Dense(10, input_shape=(9,), activation='relu'),  \n","        tf.keras.layers.Dense(1)\n","    ])\n","\n","# The Keras model in a TFF learning model\n","def model_fn():\n","    keras_model = create_keras_model()\n","    return tff.learning.models.from_keras_model(\n","        keras_model,\n","        input_spec=client_datasets[0].element_spec,\n","        loss=tf.keras.losses.MeanSquaredError(),\n","        metrics=[tf.keras.metrics.MeanSquaredError()]\n","    )\n","\n","# Defining the federated averaging process (Averaging Client to Server)\n","federated_averaging = tff.learning.algorithms.build_weighted_fed_avg(\n","    model_fn,\n","    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n","    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",")\n","\n","# Running the federated averaging process\n","state = federated_averaging.initialize()\n","for _ in range(10):\n","    state, metrics = federated_averaging.next(state, client_datasets)\n","    print('metrics:', metrics)"]},{"cell_type":"markdown","metadata":{"id":"JnX-_QEsKj1g"},"source":["### Applying the correct client format for a true federated simulation:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkDcuojVZcXR"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","scaler = StandardScaler()\n","\n","# Applying scaling to the numerical columns\n","df[['age', 'bmi', 'children', 'charges']] = scaler.fit_transform(df[['age', 'bmi', 'children', 'charges']])\n","\n","\n","# One-hot encoding categorical columns\n","df = pd.get_dummies(df, columns=['sex', 'smoker', 'region'])\n","\n","# Dropping SSN column\n","df = df.drop(columns=['SSN'])\n","\n","# Splitting data into three parts for three clients\n","df_naif, df_han, df_tamara = np.array_split(df, 3)\n","\n","# Converting to TensorFlow datasets for each client: https://www.tensorflow.org/guide/data\n","tf_dataset_naif = tf.data.Dataset.from_tensor_slices((df_naif.drop(columns=['charges']).values, df_naif['charges'].values)).batch(1)\n","tf_dataset_han = tf.data.Dataset.from_tensor_slices((df_han.drop(columns=['charges']).values, df_han['charges'].values)).batch(1)\n","tf_dataset_tamara = tf.data.Dataset.from_tensor_slices((df_tamara.drop(columns=['charges']).values, df_tamara['charges'].values)).batch(1)\n","\n","# Creating a federated dataset from the individual client datasets: https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification\n","federated_data = [tf_dataset_naif, tf_dataset_han, tf_dataset_tamara]\n","\n","# Storing the datasets in a dictionary for easy access\n","client_datasets = {\n","    'Naif': tf_dataset_naif,\n","    'Han': tf_dataset_han,\n","    'Tamara': tf_dataset_tamara,\n","}"]},{"cell_type":"markdown","metadata":{"id":"nCmoRvPsh02a"},"source":["Added or Changed:\n","*   Feature scaling\n","*   Learning rate\n","*   Model Complexity\n","*   Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkQdYlDAddXL","outputId":"d3369d62-3f1c-478c-f6fa-f4fe03716b64"},"outputs":[],"source":["# Defining a more complex Keras model \n","def create_keras_model():\n","    return tf.keras.models.Sequential([\n","        tf.keras.layers.Dense(64, activation='relu', input_shape=(11,)),\n","        tf.keras.layers.Dense(32, activation='relu'),\n","        tf.keras.layers.Dense(1)\n","    ])\n","\n","\n","# Defining a loss function changed it from MSE to MAE\n","loss_fn = tf.keras.losses.MeanAbsoluteError()\n","\n","# Creating a sample batch to infer input types\n","sample_spec = tf_dataset_naif.element_spec\n","\n","# Defining a TFF model \n","def model_fn():\n","    keras_model = create_keras_model()\n","    return tff.learning.models.from_keras_model(\n","        keras_model,\n","        input_spec=sample_spec,\n","        loss=loss_fn,\n","        metrics=[tf.keras.metrics.MeanAbsoluteError()]\n","    )\n","# Building the federated averaging process tff.learning.algorithms.build_weighted_fed_avg changed the \n","# learning rate from 0.1 to 0.01 for better metrics \n","# To delay convergence \n","fed_avg = tff.learning.algorithms.build_weighted_fed_avg(model_fn, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.01))\n","\n","\n","# Initializing state\n","state = fed_avg.initialize()\n","\n","# Defining total number of rounds\n","TOTAL_ROUNDS = 100\n","\n","# Initializing the lists to collect metrics\n","losses = []\n","mae = []\n","\n","# Looping for all rounds\n","for round_num in range(1, TOTAL_ROUNDS+1):\n","    state, metrics = fed_avg.next(state, federated_data)\n","    print('round {:2d}, metrics={}'.format(round_num, metrics))\n","    losses.append(metrics['client_work']['train']['loss'])\n","    mae.append(metrics['client_work']['train']['loss'])\n","\n","# Plotting the loss over each round\n","plt.figure()\n","plt.plot(range(1, TOTAL_ROUNDS+1), losses)\n","plt.title('Loss over epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","\n","# Plotting the mean absolute error over each round\n","plt.figure()\n","plt.plot(range(1, TOTAL_ROUNDS+1), mae)  # Mean Squared Error couldn't work due to outliers\n","plt.title('Mean Absolute Error over epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('MAE')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1cnom_FQGdiD"},"source":["### What worked:\n","Creation of a federated dataset: We have prepared our data in a federated manner, by splitting it across multiple clients and converting these splits into tf.data.Dataset instances.\n","\n","Creation of a federated learning model: We defined a function to create a Keras model and then converted this into a TFF model using the tff.learning.models.from_keras_model function.\n","\n","Running a federated learning algorithm: We used tff.learning.algorithms.build_weighted_fed_avg to create a federated learning algorithm, which we then run for multiple rounds of training."]},{"cell_type":"markdown","metadata":{"id":"TZWZep3KGQLC"},"source":["### What could be improved:\n","\n","Client Selection: The current implementation assumes that all clients are available for each round of training, which may be different in a realistic federated learning scenario. We should implement client selection strategies to select a subset of clients for each training round. \n","\n","Client Weighting: All clients contribute equally to the model update in the current code. In practice, we should assign different weights to different clients based on factors like the amount of local data, the quality of local data, or the client's contribution to the global model's performance. \n","\n","Secure Aggregation: Secure aggregation is an essential aspect of federated learning which allows the server to aggregate model updates from clients without being able to inspect individual updates, thereby enhancing privacy. This needs to be covered in our current implementation. \n","\n","Differential Privacy: Differential Privacy is another important concept in Federated Learning that helps preserve the privacy of the client's data. We may include mechanisms for adding noise to model updates to ensure differential privacy, which was already done as a standalone. \n","\n","Handling Non-IID data: In a realistic federated learning scenario, data across different clients may be non-IID (Independent and Identically Distributed). This poses several challenges, and there are various strategies to handle such systems which we should consider. \n","\n","Communication Efficiency: Communication over the network can be a bottleneck in Federated Learning. Techniques to reduce the communication cost, such as model compression like Tensorlite, could be considered. \n","\n","Model Personalization: The current implementation aims for a global model that performs well on all clients. However, in some scenarios, allowing for local model adaptations for personalization could be beneficial."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNnpOGvLCeed"},"outputs":[],"source":["       (1) Initialize\n","             |\n","             v\n","   +---------+---------+     +---------+---------+     +---------+---------+\n","   |    Client 1       |     |    Client 2       |     |    Client N       |\n","   | - Local dataset   |     | - Local dataset   |     | - Local dataset   |\n","   | - Copy of model   |     | - Copy of model   |     | - Copy of model   |\n","   +---------+---------+     +---------+---------+     +---------+---------+\n","             | (2) Broadcast  | (2) Broadcast  | (2) Broadcast\n","             | Model Params   | Model Params   | Model Params\n","             v                v                v\n","             | (3) Local      | (3) Local      | (3) Local\n","             | Computation    | Computation    | Computation\n","             |                |                |\n","+------------+-------+     +--+------------+   |  +---------+---------+\n","| Updated Model Params |     | Updated Model Params |  | Updated Model Params |\n","+------------+-------+     +--+------------+   |  +---------+---------+\n","             | (4) Aggregation | (4) Aggregation | (4) Aggregation\n","             +-----------------+-----------------+-----------------+\n","                                       |\n","                                       v\n","                              (5) Global Model Update\n","                                       |\n","                                       v\n","                                  (6) iIterate\n"]},{"cell_type":"markdown","metadata":{"id":"VWsrZpAA356M"},"source":["\"Data Scaling\" represents the step of scaling the numerical features in the dataset.\n","\n","\"One-Hot Encoding\" indicates the process of converting categorical features into a binary vector representation.\n","\n","\"Data Splitting\" represents the division of data into different parts for each client in the federated learning setting.\n","\n","\"Federated Data\" indicates the dataset used for each client in the federated learning process.\n","\n","\"Keras Model\" represents the machine learning model built using the Keras library.\n","\n","\"Loss Function\" represents the function used to compute the loss or error of the model.\n","\n","\"TFF Model\" represents the TensorFlow Federated (TFF) model, which adapts the Keras model for federated learning.\n","\n","\"Federated Averaging\" represents the federated averaging algorithm used to train the model collaboratively across clients.\n","\n","\"Training Loop\" represents the iterative training process of the federated averaging algorithm.\n","\n","\"Losses\" indicates the loss values during the training process.\n","\n","\"Mean Absolute Error\" represents the mean absolute error metric used to evaluate the model's performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aHslkJ0IEMQ","outputId":"b8fef4b7-93ee-422f-8452-aaa49c492dc3"},"outputs":[],"source":["# Creat nodes\n","nodes = [\n","    (\"Initialize\", (0, 4)),\n","    (\"Naif\", (2, 2)),\n","    (\"Han\", (2, 4)),\n","    (\"Tamara\", (2, 6)),\n","    (\"Broadcast\\nModel Params\", (4, 4)),\n","    (\"Local\\nComputation\", (6, 2)),\n","    (\"Local\\nComputation\", (6, 4)),\n","    (\"Local\\nComputation\", (6, 6)),\n","    (\"Updated Model\\nParams\", (8, 2)),\n","    (\"Updated Model\\nParams\", (8, 4)),\n","    (\"Updated Model\\nParams\", (8, 6)),\n","    (\"Aggregation\", (10, 4)),\n","    (\"Global Model\\nUpdate\", (12, 4)),\n","    (\"Iterate\", (14, 4))\n","]\n","\n","edges = [\n","    ((0, 4), (2, 2)), ((0, 4), (2, 4)), ((0, 4), (2, 6)),\n","    ((2, 2), (4, 4)), ((2, 4), (4, 4)), ((2, 6), (4, 4)),\n","    ((4, 4), (6, 2)), ((4, 4), (6, 4)), ((4, 4), (6, 6)),\n","    ((6, 2), (8, 2)), ((6, 4), (8, 4)), ((6, 6), (8, 6)),\n","    ((8, 2), (10, 4)), ((8, 4), (10, 4)), ((8, 6), (10, 4)),\n","    ((10, 4), (12, 4)), ((12, 4), (14, 4))\n","]\n","\n","\n","fig, ax = plt.subplots(figsize=(16, 6))\n","ax.set_xlim([-1, 15])\n","ax.set_ylim([-1, 7])\n","for node, pos in nodes:\n","    ax.text(pos[0], pos[1], node, ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\",\n","            bbox=dict(boxstyle=\"round\", facecolor=\"white\"))\n","\n","for start, end in edges:\n","    ax.annotate(\"\", xy=start, xytext=end, arrowprops=dict(arrowstyle=\"->\", linewidth=1, color=\"gray\"))\n","\n","\n","ax.set_title(\"Federated Learning Process\", fontsize=16, fontweight=\"bold\")\n","ax.set_xticks([])\n","ax.set_yticks([])\n","ax.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"IpruCDBybVTu"},"source":["## Federated Learning Process\n","\n","[Federated Learning](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html) is a machine learning setting where multiple devices (clients) collaboratively learn a shared model while keeping all the training data on the original device, decoupling the ability to do machine learning from the need to store the data in the cloud. This has the advantage of privacy by design: The raw data is never exposed to the server, and sensitive information remains on the device. Here is the general flow of the Federated Learning process:\n","\n","1. **Initialization**: The server initializes the global model parameters $w$.\n","\n","2. **Broadcast**: The server sends these parameters $w$ to all participating devices (clients).\n","\n","3. **Local Computation**: Each client computes an update to the model parameters based on its local data. Specifically, the client computes a gradient of the loss function with respect to the model parameters. Let $x_i$ represent the local data for client $i$ and $y_i$ represent the labels. The client computes a local update $\\delta_i$ as follows:\n","$$\\delta_i = -\\eta \\nabla_w L(w;x_i, y_i)$$\n","where $L$ is the loss function, $\\eta$ is the learning rate, and $\\nabla$ denotes the gradient.\n","\n","4. **Send Model Updates**: Each client sends its computed update $\\delta_i$ back to the server.\n","\n","5. **Aggregation**: The server aggregates the updates from each client to compute an overall update. The simplest way to do this is to compute an average:\n","$$\\Delta = \\frac{1}{n}\\sum_{i=1}^{n}\\delta_i$$\n","where $n$ is the total number of clients in our case 3.\n","\n","6. **Global Model Update**: The server updates the global model parameters based on the aggregated update:\n","$$w = w + \\Delta$$\n","\n","7. **Iterate**: The process repeats from step 2 until convergence, i.e., until the change in the global model parameters is smaller than a specified threshold.\n","\n","For more detailed information, you can refer to [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629) and [Advances and Open Problems in Federated Learning](https://arxiv.org/abs/1912.04977).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__PxuXB7RO7c","outputId":"fc4226c1-1f80-404c-ebb9-14d0cd60e2fb"},"outputs":[],"source":["!apt-get install -y graphviz libgraphviz-dev\n","!pip install pygraphviz\n","!pip install pydot pydotplus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaHVmyVUmvEh","outputId":"6a6a202b-3a8c-4363-a76f-2a7c04d6ac7b"},"outputs":[],"source":["import networkx as nx\n","\n","G = nx.DiGraph()\n","\n","nodes = {\n","'start': 'Initialize $w_i = w_{global}$',\n","'local_comp': r'Local Computation $grad = \\nabla w_i L(w_i, x_i, y_i)$',\n","'update': 'Update $\\delta_i = -\\eta * grad$',\n","'updated_model_params': 'Updated Model Params $w_i = w_i + \\delta_i$',\n","'aggregation': 'Aggregation $\\Delta = \\frac{1}{n}\\sum_{i=1}^{n}\\delta_i$',\n","'global_update': 'Global Model Update $w_{global} = w_{global} + \\Delta$',\n","'iterate': 'Iterate for epochs'\n","}\n","\n","for node, label in nodes.items():\n","  G.add_node(node, label=label)\n","\n","edges = [\n","('start', 'local_comp'),\n","('local_comp', 'update'),\n","('update', 'updated_model_params'),\n","('updated_model_params', 'aggregation'),\n","('aggregation', 'global_update'),\n","('global_update', 'iterate'),\n","('iterate', 'start')\n","]\n","G.add_edges_from(edges)\n","\n","edge_labels = {\n","('start', 'local_comp'): r'$1...n$',\n","('local_comp', 'update'): '',\n","('update', 'updated_model_params'): '',\n","('updated_model_params', 'aggregation'): r'$1...n$',\n","('aggregation', 'global_update'): '',\n","('global_update', 'iterate'): '',\n","('iterate', 'start'): r'$epoch < N$'\n","}\n","\n","node_sizes = [60000 + len(label) * 250 for label in nx.get_node_attributes(G, 'label').values()]\n","\n","color_palette = ['#84C2F2', '#4F81BD', '#C0504D', '#9BBB59', '#8064A2', '#4BACC6', '#F79646']\n","colors = [color_palette[i % len(color_palette)] for i in range(len(G))]\n","\n","plt.figure(figsize=(24, 20))\n","pos = nx.circular_layout(G) # Use circular layout for the graph\n","nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=node_sizes, node_shape='o', edgecolors='k', linewidths=2.0)\n","nx.draw_networkx_labels(G, pos, labels=nx.get_node_attributes(G, 'label'), font_size=12, font_weight='bold', font_family='serif', verticalalignment='center')\n","nx.draw_networkx_edges(G, pos, connectionstyle='arc3,rad=0.1', edge_color='black', width=4, arrowstyle='-|>', alpha=0.9)\n","nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=18, font_family='serif')\n","plt.axis('off')\n","plt.margins(0.1)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ayjuqdNSsAa5","outputId":"f9c26be8-8f36-436c-e487-126baa5bf370"},"outputs":[],"source":["!pip install dot2tex"]},{"cell_type":"markdown","metadata":{"id":"_M2Y3nObklIP"},"source":["### Explanation\n","1.   Data Preparation: The script begins by manipulating the dataset. Numerical columns ('age', 'bmi', 'children', and 'charges') are standardized using sklearn's StandardScaler. This ensures that these features have a mean value of 0 and a standard deviation of 1, reducing the sensitivity of the model to varying scales. The script also converts categorical columns ('sex', 'smoker', and 'region') into binary vectors via one-hot encoding. An unneeded column, 'SSN', is subsequently discarded.\n","\n","2.   Data Segregation: The processed data is then divided into three segments, each intended for a different participant ('Naif', 'Han', 'Tamara'). This aligns with the concept of federated learning, where each participant has a unique subset of the overall data.\n","\n","3.   TensorFlow Dataset Creation: The script transforms each client's data into a TensorFlow dataset, which is then batched. The model's inputs are all columns excluding 'charges', while 'charges' is set as the target variable.\n","\n","4.   Formation of Federated Dataset: All the individual client datasets are gathered into a list to form a federated dataset.\n","\n","5.   Model Architecture: Using Keras, the script defines a neural network model with two dense layers (using ReLU activation) and a final output layer (without any activation function), typically used in regression problems.\n","\n","6.   TFF Model Initialization: The Keras model is wrapped into a TFF model with the help of the tff.learning.from_keras_model function. The model uses mean absolute error (MAE) as its loss function and mean squared error (MSE) as its performance metric.\n","\n","7.   Setting up Federated Learning Process: The script sets up the federated learning process by calling the tff.learning.build_federated_averaging_process function. This function establishes a federated averaging process, enabling the model to learn from the federated data. The client optimizer chosen for this process is the stochastic gradient descent (SGD) with a learning rate of 0.01.\n","\n","8.   Model Training: The model training phase consists of a certain number of rounds. In each round, the script calls the 'next' method of the federated averaging process. This method executes one federated averaging step, which involves training the model on each client's data, transmitting the model updates to the server, and averaging these updates.\n","\n","9.   Model Evaluation: After each round of training, the script logs the model's loss and MAE. These values are subsequently visualized to demonstrate how the model's performance evolves over the training rounds."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WxTaBLPQ5fYL","outputId":"97b80855-58a8-459d-dbc8-c7eb7eae792f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pydot in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.2)\n","Collecting pydotplus\n","  Downloading pydotplus-2.0.2.tar.gz (278 kB)\n","     ---------------------------------------- 0.0/278.7 kB ? eta -:--:--\n","     ---- -------------------------------- 30.7/278.7 kB 445.2 kB/s eta 0:00:01\n","     ---- -------------------------------- 30.7/278.7 kB 445.2 kB/s eta 0:00:01\n","     ---- -------------------------------- 30.7/278.7 kB 445.2 kB/s eta 0:00:01\n","     ---- -------------------------------- 30.7/278.7 kB 445.2 kB/s eta 0:00:01\n","     --------- --------------------------- 71.7/278.7 kB 302.7 kB/s eta 0:00:01\n","     -------------- --------------------- 112.6/278.7 kB 409.6 kB/s eta 0:00:01\n","     -------------- --------------------- 112.6/278.7 kB 409.6 kB/s eta 0:00:01\n","     --------------- -------------------- 122.9/278.7 kB 343.4 kB/s eta 0:00:01\n","     ------------------------- ---------- 194.6/278.7 kB 491.5 kB/s eta 0:00:01\n","     ------------------------------ ----- 235.5/278.7 kB 535.1 kB/s eta 0:00:01\n","     ------------------------------ ----- 235.5/278.7 kB 535.1 kB/s eta 0:00:01\n","     -----------------------------------  276.5/278.7 kB 532.5 kB/s eta 0:00:01\n","     ------------------------------------ 278.7/278.7 kB 521.2 kB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\adity\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydot) (2.4.7)\n","Building wheels for collected packages: pydotplus\n","  Building wheel for pydotplus (setup.py): started\n","  Building wheel for pydotplus (setup.py): finished with status 'done'\n","  Created wheel for pydotplus: filename=pydotplus-2.0.2-py3-none-any.whl size=24574 sha256=a97e9fce8ce05b9ed7bb90e0b599113f7d710a9e332b5b751eb4523b1afa69cf\n","  Stored in directory: c:\\users\\adity\\appdata\\local\\pip\\cache\\wheels\\bd\\ce\\e8\\ff9d9c699514922f57caa22fbd55b0a32761114b4c4acc9e03\n","Successfully built pydotplus\n","Installing collected packages: pydotplus\n","Successfully installed pydotplus-2.0.2\n"]}],"source":["!pip install pydot pydotplus"]},{"cell_type":"markdown","metadata":{"id":"UsRnObqg52GF"},"source":["* !apt-get install -y graphviz libgraphviz-dev\n","* !pip install pygraphviz\n","* !pip install pydot pydotplus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWk6tjGTPBmf","outputId":"7eed3579-9751-4c52-8af3-7c8391f37a2d"},"outputs":[],"source":["import networkx as nx\n","from networkx.drawing.nx_agraph import graphviz_layout\n","import textwrap\n","\n","# Defining the nodes and their attributes\n","nodes = {\n","    \"Data Scaling\": {\"color\": \"pink\", \"shape\": \"box\", \"size\": 2000},\n","    \"One-Hot Encoding\": {\"color\": \"pink\", \"shape\": \"box\", \"size\": 2000},\n","    \"Data Splitting\": {\"color\": \"pink\", \"shape\": \"box\", \"size\": 2000},\n","    \"Federated Data\": {\"color\": \"pink\", \"shape\": \"box\", \"size\": 2000},\n","    \"Keras Model\": {\"color\": \"lightblue\", \"shape\": \"ellipse\", \"size\": 2000},\n","    \"Loss Function\": {\"color\": \"lightblue\", \"shape\": \"ellipse\", \"size\": 2000},\n","    \"TFF Model\": {\"color\": \"lightblue\", \"shape\": \"ellipse\", \"size\": 2000},\n","    \"Federated Averaging\": {\"color\": \"lightgreen\", \"shape\": \"diamond\", \"size\": 2000},\n","    \"Training Loop\": {\"color\": \"lightgreen\", \"shape\": \"diamond\", \"size\": 2000},\n","    \"Losses\": {\"color\": \"orange\", \"shape\": \"hexagon\", \"size\": 2000},\n","    \"Mean Absolute Error\": {\"color\": \"orange\", \"shape\": \"hexagon\", \"size\": 2000},\n","}\n","\n","# Creating a new graph\n","graph = nx.DiGraph()\n","\n","# Adding nodes to the graph\n","for node, attributes in nodes.items():\n","    graph.add_node(node, **attributes)\n","\n","# Defining the edges and their attributes\n","edges = {\n","    (\"Data Scaling\", \"One-Hot Encoding\"): {\"label\": \"Transforms\"},\n","    (\"One-Hot Encoding\", \"Data Splitting\"): {\"label\": \"Encodes\"},\n","    (\"Data Splitting\", \"Federated Data\"): {\"label\": \"Splits\"},\n","    (\"Federated Data\", \"TFF Model\"): {\"label\": \"Feeds\"},\n","    (\"Keras Model\", \"TFF Model\"): {\"label\": \"Defines\"},\n","    (\"Loss Function\", \"TFF Model\"): {\"label\": \"Determines\"},\n","    (\"Federated Averaging\", \"Training Loop\"): {\"label\": \"Averages\"},\n","    (\"TFF Model\", \"Training Loop\"): {\"label\": \"Trains\"},\n","    (\"Training Loop\", \"Losses\"): {\"label\": \"Produces\"},\n","    (\"Training Loop\", \"Mean Absolute Error\"): {\"label\": \"Calculates\"},\n","}\n","\n","# Adding edges to the graph\n","for edge, attributes in edges.items():\n","    graph.add_edge(*edge, **attributes)\n","\n","# Graphing layout setting\n","pos = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n","\n","# Specifying the positions of the nodes\n","node_positions = {\n","    \"Data Scaling\": (59.5, 540.0),\n","    \"One-Hot Encoding\": (59.5, 453.0),\n","    \"Data Splitting\": (59.5, 366.0),\n","    \"Federated Data\": (59.5, 279.0),\n","    \"Keras Model\": (184.5, 279.0),\n","    \"Loss Function\": (320.5, 279.0),\n","    \"TFF Model\": (184.5, 192.0),\n","    \"Federated Averaging\": (373.5, 192.0),\n","    \"Training Loop\": (279.5, 105.0),\n","    \"Losses\": (201.5, 18.0),\n","    \"Mean Absolute Error\": (358.5, 18.0),\n","}\n","\n","# Creating a new figure with the desired size\n","plt.figure(figsize=(20, 18))\n","\n","# Increasing the size of the node circles\n","node_sizes = [node[1][\"size\"] * 3 for node in graph.nodes(data=True)]\n","\n","# Drawing the nodes with specified positions\n","nx.draw_networkx_nodes(\n","    graph,\n","    pos=node_positions,\n","    node_color=[node[1][\"color\"] for node in graph.nodes(data=True)],\n","    node_size=node_sizes,\n","    alpha=0.9,\n",")\n","\n","# Drawing the node labels\n","labels = {node: textwrap.fill(node, width=10) for node in graph.nodes()}\n","nx.draw_networkx_labels(graph, node_positions, labels=labels, font_size=12)\n","\n","# Drawing the edges\n","nx.draw_networkx_edges(graph, pos, arrows=True, arrowstyle=\"->\", arrowsize=10)\n","\n","# Drawing the edge labels\n","edge_labels = nx.get_edge_attributes(graph, \"label\")\n","nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=12)\n","plt.title('Flowchart of the Federated Learning Architecture', fontsize=32)\n","plt.axis('off')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"O0v3Y5f1WmS9"},"source":["## EHR Archeticture with Diagram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iwpyR_hyWvoF"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dV2vRPQ6XglS"},"outputs":[],"source":["def data_tester(df):\n","    display(df)\n","    print()\n","\n","    shape = df.shape\n","    display(shape)\n","    print()\n","\n","    display('Missing Values:')\n","    missing_values = df.isnull().sum()\n","    display(missing_values)\n","    print()\n","\n","    display('Data Types:')\n","    data_types = df.dtypes\n","    display(data_types)\n","    print()\n","\n","    return df, shape, missing_values, data_types"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZK_3s0AWpOk","outputId":"41d0513b-4b32-4050-c083-2c60c7fc52f2"},"outputs":[],"source":["df = pd.read_csv('/content/insurance_ssn.csv')\n","data_tester(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crZRn4XdyNPI"},"outputs":[],"source":["import uuid\n","import datetime\n","from pandas import DataFrame\n","import csv\n","\n","# Load the dataset\n","df: DataFrame = pd.read_csv('/content/insurance_ssn.csv')\n","\n","# Anonymize the data by replacing SSNs with random UUIDs\n","df['SSN'] = [uuid.uuid4() for _ in range(len(df))]\n","\n","class Logger:\n","    def __init__(self, csv_filename=\"log.csv\"):\n","        self.csv_filename = csv_filename\n","        self.log_file = open(self.csv_filename, 'a', newline='')\n","        self.writer = csv.writer(self.log_file)\n","        self.is_first_entry = self.log_file.tell() == 0\n","\n","    def log(self, message):\n","        timestamp = datetime.datetime.now()\n","        print(f'[{timestamp}]: {message}')\n","        if self.is_first_entry:\n","            self.writer.writerow([\"Timestamp\", \"Message\"])\n","            self.is_first_entry = False\n","        self.writer.writerow([str(timestamp), message])\n","\n","    def close(self):\n","        self.log_file.close()\n","\n","class Role:\n","    def __init__(self, name, permissions):\n","        self.name = name\n","        self.permissions = permissions\n","\n","class User:\n","    def __init__(self, name, role):\n","        self.name = name\n","        self.role = role\n","\n","class Record:\n","    def __init__(self, data, logger):\n","        self.data = data\n","        self.third_party_permissions = []\n","        self.logger = logger\n","\n","class Patient:\n","    def __init__(self, name, age, logger):\n","        self.name = name\n","        self.age = age\n","        self.logger = logger\n","\n","class PatientRecord:\n","    def __init__(self, patient, data):\n","        self.patient = patient\n","        self.data = data\n","        self.logger = patient.logger\n","\n","    def add_data(self, new_data):\n","        self.data.update(new_data)\n","        self.logger.log(f'New data added for patient {self.patient.name}: {new_data}')\n","\n","class Doctor:\n","    def __init__(self, name, logger):\n","        self.name = name\n","        self.logger = logger\n","\n","    def notify(self, message):\n","        self.logger.log(f\"Doctor {self.name}, {message}\")\n","\n","class HealthRiskAssessment:\n","    def __init__(self, record):\n","        self.record = record\n","\n","    def assess_bmi_and_update_record(self, new_bmi, doctor):\n","        self.record.add_data({'bmi': new_bmi})\n","        self.alert_if_at_risk(doctor)\n","\n","    def alert_if_at_risk(self, doctor):\n","        bmi = self.record.data.get('bmi')\n","        if bmi is not None and (bmi < 18.5 or bmi > 25):\n","            doctor.notify(f\"Patient {self.record.patient.name} is at risk due to BMI of {bmi}\")\n","\n","class Hospital:\n","    def __init__(self, name, logger):\n","        self.name = name\n","        self.doctors = []\n","        self.patients = []\n","        self.logger = logger\n","\n","    def add_doctor(self, doctor):\n","        if doctor not in self.doctors:\n","            self.doctors.append(doctor)\n","            self.logger.log(f'Doctor {doctor.name} has joined the hospital {self.name}.')\n","\n","    def add_patient(self, patient):\n","        if patient not in self.patients:\n","            self.patients.append(patient)\n","            self.logger.log(f'Patient {patient.name} has joined the hospital {self.name}.')\n","\n","    def assign_doctor_to_patient(self, doctor, patient):\n","        if doctor in self.doctors and patient in self.patients:\n","            patient_record = PatientRecord(patient, {'doctor': doctor.name})\n","            self.logger.log\n","            (f'Doctor {doctor.name} has been assigned to patient {patient.name}.')\n","            return patient_record\n","        else:\n","            self.logger.log(f'Error: Doctor {doctor.name} and/or patient {patient.name} are not part of the hospital {self.name}.')\n","\n","class Pharmacy:\n","    def __init__(self, name):\n","        self.name = name\n","        self.medication_stock = {}\n","        self.logger = Logger()\n","\n","    def add_medication(self, medication, quantity):\n","        if medication in self.medication_stock:\n","            self.medication_stock[medication] += quantity\n","        else:\n","            self.medication_stock[medication] = quantity\n","            self.logger.log(f'{quantity} units of {medication} added to the pharmacy {self.name}.')\n","\n","    def dispense_medication(self, medication, quantity):\n","        if medication in self.medication_stock and self.medication_stock[medication] >= quantity:\n","            self.medication_stock[medication] -= quantity\n","            self.logger.log(f'{quantity} units of {medication} dispensed from the pharmacy {self.name}.')\n","        else:\n","            self.logger.log(f'Error: Insufficient stock of {medication} in the pharmacy {self.name}.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4YqCrYdd1Oq","outputId":"82c4b977-d8aa-452e-c482-15c53e2cf2fd"},"outputs":[],"source":["# Testing the Classes Part 1: \n","# ------------------------------------------------------------------------------\n","\n","# Initializing the Logger object before creating the rest of the data\n","logger = Logger('hospital_1_log.csv')\n","\n","# Creating a role with rules and responsibilities and a user\n","admin_role = Role('admin', ['read', 'write', 'delete'])\n","admin_user = User('Prof. Tamara', admin_role)\n","\n","# Printing user name and the role\n","print(f'User: {admin_user.name}, Role: {admin_user.role.name}')\n","\n","# Creating a Doctor\n","doctor_1 = Doctor('Dr. Han', logger)\n","\n","# Creating a hospital and adding the patient and the doctor to it\n","hospital_1 = Hospital('UW Medical Center',logger)\n","hospital_1.add_doctor(doctor_1)\n","\n","# Creating a Patient and assigning the doctor to the patient\n","# Pass the logger object explicitly\n","patient_1 = Patient('Naif', 25, logger)  \n","\n","hospital_1.add_patient(patient_1)\n","patient_record_1 = hospital_1.assign_doctor_to_patient(doctor_1, patient_1)\n","\n","patient_record_1.add_data({'weight': 85, 'height': 1.85})\n","\n","# Creating a health risk assessment for the patient based on BMI\n","hra_1 = HealthRiskAssessment(patient_record_1)\n","\n","# Calculating BMI (weight in kg / (height in m)^2) and updating patient's record\n","bmi = patient_record_1.data.get('weight') / (patient_record_1.data.get('height') ** 2)\n","hra_1.assess_bmi_and_update_record(bmi, doctor_1)\n","\n","# Working in Progress with Pharmacy\n","pharmacy_1 = Pharmacy('UW Medical Center Pharmacy')\n","pharmacy_1.add_medication('Iburprofen', 500)\n","pharmacy_1.dispense_medication('Aspirin', 50)\n","\n","# Closing the Logger\n","logger.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6Fh1BacVWHC","outputId":"14372447-f472-4bfd-8e3a-fcd3f357350f"},"outputs":[],"source":["# Testing the Classes Part 2: \n","# ------------------------------------------------------------------------------\n","\n","# Initializing the Logger object before creating the rest of the data\n","logger = Logger('hospital_2_log.csv')\n","\n","# Creating a role with rules and responsibilities and a user\n","patient_role = Role('patient', ['read'])\n","patient_user = User('Zack', patient_role)\n","\n","# Printing user name and the role\n","print(f'User: {patient_user.name}, Role: {patient_user.role.name}')\n","\n","# Creating a Doctor\n","doctor_2 = Doctor('Dr. Steve', logger)\n","\n","# Creating a hospital and adding the patient and the doctor to it\n","hospital_2 = Hospital('Seattle Hospital', logger)\n","hospital_2.add_doctor(doctor_2)\n","\n","# Creating a Patient and assigning the doctor to the patient\n","# Pass the logger object explicitly\n","patient_2 = Patient('Zack', 35, logger)  \n","\n","hospital_2.add_patient(patient_2)\n","patient_record_2 = hospital_2.assign_doctor_to_patient(doctor_2, patient_2)\n","\n","patient_record_2.add_data({'weight': 60, 'height': 1.55})\n","\n","# Creating a health risk assessment for the patient based on BMI\n","hra_2 = HealthRiskAssessment(patient_record_2)\n","\n","# Calculating BMI (weight in kg / (height in m)^2) and updating patient's record\n","bmi = patient_record_2.data.get('weight') / (patient_record_2.data.get('height') ** 2)\n","hra_2.assess_bmi_and_update_record(bmi, doctor_2)\n","\n","# Working in Progress with Pharmacy\n","pharmacy_2 = Pharmacy('Seattle Pharmacy')\n","pharmacy_2.add_medication('XYZ', 500)\n","pharmacy_2.dispense_medication('XYZ', 50)\n","\n","# Closing the Logger\n","logger.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkx_UTSHXA_f"},"outputs":[],"source":["hs_1_log = pd.read_csv('/content/hospital_1_log.csv')\n","hs_2_log = pd.read_csv('/content/hospital_2_log.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-dfsa4-XiS7","outputId":"2477c97b-e2f6-4ef0-ee9e-1723451bb4b0"},"outputs":[],"source":["pd.set_option('display.max_colwidth', None) # To showcase the full message\n","display(hs_1_log)\n","print()\n","display(hs_2_log)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGRUKhbohZQQ","outputId":"f61120b7-7496-4b0c-c2d2-312ca76e30da"},"outputs":[],"source":["log = pd.read_csv('/content/log.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IIl9EnhYQ_l0","outputId":"7abab7b1-adb8-4ff6-92d0-e184d1156ec7"},"outputs":[],"source":["import networkx as nx\n","import matplotlib.pyplot as plt\n","\n","# Creating a new graph\n","graph = nx.DiGraph()\n","\n","# Adding nodes for each class\n","graph.add_node(\"Logger\")\n","graph.add_node(\"Role\")\n","graph.add_node(\"User\")\n","graph.add_node(\"Record\")\n","graph.add_node(\"Patient\")\n","graph.add_node(\"PatientRecord\")\n","graph.add_node(\"Doctor\")\n","graph.add_node(\"HealthRiskAssessment\")\n","graph.add_node(\"Hospital\")\n","graph.add_node(\"Pharmacy\")\n","\n","# Adding edges\n","graph.add_edge(\"Logger\", \"Record\", label=\"Logs\")\n","graph.add_edge(\"Logger\", \"Doctor\", label=\"Logs\")\n","graph.add_edge(\"Logger\", \"Patient\", label=\"Logs\")\n","graph.add_edge(\"Logger\", \"PatientRecord\", label=\"Logs\")\n","graph.add_edge(\"Role\", \"User\", label=\"Manages\")\n","graph.add_edge(\"User\", \"Logger\", label=\"Accesses\")\n","graph.add_edge(\"Record\", \"Patient\", label=\"Belongs to\")\n","graph.add_edge(\"PatientRecord\", \"Patient\", label=\"Includes\")\n","graph.add_edge(\"PatientRecord\", \"Record\", label=\"Includes\")\n","graph.add_edge(\"Doctor\", \"Logger\", label=\"Logs\")\n","graph.add_edge(\"Doctor\", \"Hospital\", label=\"Works at\")\n","graph.add_edge(\"Doctor\", \"HealthRiskAssessment\", label=\"Performs\")\n","graph.add_edge(\"HealthRiskAssessment\", \"Record\", label=\"Generates\")\n","graph.add_edge(\"Hospital\", \"Doctor\", label=\"Has doctors\")\n","graph.add_edge(\"Hospital\", \"Patient\", label=\"Treats\")\n","graph.add_edge(\"Hospital\", \"PatientRecord\", label=\"Has records\")\n","graph.add_edge(\"Pharmacy\", \"Logger\", label=\"Logs\")\n","\n","node_attributes = {\n","    \"shape\": \"box\",\n","    \"style\": \"filled\",\n","    \"fillcolor\": \"white\",  \n","}\n","nx.set_node_attributes(graph, node_attributes)\n","\n","plt.figure(figsize=(24, 18))\n","\n","# Setting the node colors\n","node_colors = [\"pink\", \"lightblue\", \"lightgreen\", \"orange\", \"yellow\", \"cyan\", \"violet\", \"lightgray\", \"salmon\", \"lightpink\"]\n","node_color_values = [node_colors[i % len(node_colors)] for i in range(len(graph.nodes()))]\n","\n","node_sizes = [5000 * graph.degree(node) for node in graph.nodes()]\n","\n","edge_colors = [\"gray\"] * len(graph.edges())\n","edge_styles = [\"solid\"] * len(graph.edges())\n","\n","pos = nx.shell_layout(graph)\n","\n","# Drawing the nodes\n","nx.draw_networkx_nodes(graph, pos, node_color=node_color_values, node_size=node_sizes, alpha=1.0)  # Set alpha to 1.0\n","\n","# Drawing the labels\n","nx.draw_networkx_labels(graph, pos, font_size=14, font_weight=\"bold\")\n","\n","# Drawing the edges\n","nx.draw_networkx_edges(graph, pos, edge_color=edge_colors, width=2.0, alpha=0.5, style=edge_styles, arrowsize=20)\n","\n","# Drawing the edge labels\n","edge_labels = nx.get_edge_attributes(graph, \"label\")\n","nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=12)\n","\n","# Title\n","plt.title(\"EHR Architecture\", fontsize=24)\n","plt.axis(\"off\")\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
